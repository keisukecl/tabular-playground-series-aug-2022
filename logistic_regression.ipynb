{"cells":[{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-01-11T08:10:49.844808Z","iopub.status.busy":"2022-01-11T08:10:49.843761Z","iopub.status.idle":"2022-01-11T08:10:57.334528Z","shell.execute_reply":"2022-01-11T08:10:57.333809Z","shell.execute_reply.started":"2022-01-11T08:10:49.844676Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","import numpy as np \n","import pandas as pd\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re\n","import os\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split, GroupKFold\n","#最大表示列数の指定（ここでは50列を指定）\n","pd.set_option('display.max_columns', 50)\n"]},{"cell_type":"markdown","metadata":{},"source":["訓練データとテストデータの読み込み"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-01-11T08:10:57.336251Z","iopub.status.busy":"2022-01-11T08:10:57.335921Z","iopub.status.idle":"2022-01-11T08:10:59.290014Z","shell.execute_reply":"2022-01-11T08:10:59.289245Z","shell.execute_reply.started":"2022-01-11T08:10:57.336212Z"},"trusted":true},"outputs":[],"source":["train = pd.read_csv(\"../data/train.csv\", index_col=\"id\")\n","test = pd.read_csv(\"../data/test.csv\", index_col=\"id\")\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>product_code</th>\n","      <th>loading</th>\n","      <th>attribute_0</th>\n","      <th>attribute_1</th>\n","      <th>attribute_2</th>\n","      <th>attribute_3</th>\n","      <th>measurement_0</th>\n","      <th>measurement_1</th>\n","      <th>measurement_2</th>\n","      <th>measurement_3</th>\n","      <th>measurement_4</th>\n","      <th>measurement_5</th>\n","      <th>measurement_6</th>\n","      <th>measurement_7</th>\n","      <th>measurement_8</th>\n","      <th>measurement_9</th>\n","      <th>measurement_10</th>\n","      <th>measurement_11</th>\n","      <th>measurement_12</th>\n","      <th>measurement_13</th>\n","      <th>measurement_14</th>\n","      <th>measurement_15</th>\n","      <th>measurement_16</th>\n","      <th>measurement_17</th>\n","      <th>failure</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A</td>\n","      <td>80.10</td>\n","      <td>material_7</td>\n","      <td>material_8</td>\n","      <td>9</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>8</td>\n","      <td>4</td>\n","      <td>18.040</td>\n","      <td>12.518</td>\n","      <td>15.748</td>\n","      <td>19.292</td>\n","      <td>11.739</td>\n","      <td>20.155</td>\n","      <td>10.672</td>\n","      <td>15.859</td>\n","      <td>17.594</td>\n","      <td>15.193</td>\n","      <td>15.029</td>\n","      <td>NaN</td>\n","      <td>13.034</td>\n","      <td>14.684</td>\n","      <td>764.100</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A</td>\n","      <td>84.89</td>\n","      <td>material_7</td>\n","      <td>material_8</td>\n","      <td>9</td>\n","      <td>5</td>\n","      <td>14</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>18.213</td>\n","      <td>11.540</td>\n","      <td>17.717</td>\n","      <td>17.893</td>\n","      <td>12.748</td>\n","      <td>17.889</td>\n","      <td>12.448</td>\n","      <td>17.947</td>\n","      <td>17.915</td>\n","      <td>11.755</td>\n","      <td>14.732</td>\n","      <td>15.425</td>\n","      <td>14.395</td>\n","      <td>15.631</td>\n","      <td>682.057</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>A</td>\n","      <td>82.43</td>\n","      <td>material_7</td>\n","      <td>material_8</td>\n","      <td>9</td>\n","      <td>5</td>\n","      <td>12</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>18.057</td>\n","      <td>11.652</td>\n","      <td>16.738</td>\n","      <td>18.240</td>\n","      <td>12.718</td>\n","      <td>18.288</td>\n","      <td>12.715</td>\n","      <td>15.607</td>\n","      <td>NaN</td>\n","      <td>13.798</td>\n","      <td>16.711</td>\n","      <td>18.631</td>\n","      <td>14.094</td>\n","      <td>17.946</td>\n","      <td>663.376</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>A</td>\n","      <td>101.07</td>\n","      <td>material_7</td>\n","      <td>material_8</td>\n","      <td>9</td>\n","      <td>5</td>\n","      <td>13</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>17.295</td>\n","      <td>11.188</td>\n","      <td>18.576</td>\n","      <td>18.339</td>\n","      <td>12.583</td>\n","      <td>19.060</td>\n","      <td>12.471</td>\n","      <td>16.346</td>\n","      <td>18.377</td>\n","      <td>10.020</td>\n","      <td>15.250</td>\n","      <td>15.562</td>\n","      <td>16.154</td>\n","      <td>17.172</td>\n","      <td>826.282</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>A</td>\n","      <td>188.06</td>\n","      <td>material_7</td>\n","      <td>material_8</td>\n","      <td>9</td>\n","      <td>5</td>\n","      <td>9</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>19.346</td>\n","      <td>12.950</td>\n","      <td>16.990</td>\n","      <td>15.746</td>\n","      <td>11.306</td>\n","      <td>18.093</td>\n","      <td>10.337</td>\n","      <td>17.082</td>\n","      <td>19.932</td>\n","      <td>12.428</td>\n","      <td>16.182</td>\n","      <td>12.760</td>\n","      <td>13.153</td>\n","      <td>16.412</td>\n","      <td>579.885</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   product_code  loading attribute_0 attribute_1  attribute_2  attribute_3  \\\n","id                                                                           \n","0             A    80.10  material_7  material_8            9            5   \n","1             A    84.89  material_7  material_8            9            5   \n","2             A    82.43  material_7  material_8            9            5   \n","3             A   101.07  material_7  material_8            9            5   \n","4             A   188.06  material_7  material_8            9            5   \n","\n","    measurement_0  measurement_1  measurement_2  measurement_3  measurement_4  \\\n","id                                                                              \n","0               7              8              4         18.040         12.518   \n","1              14              3              3         18.213         11.540   \n","2              12              1              5         18.057         11.652   \n","3              13              2              6         17.295         11.188   \n","4               9              2              8         19.346         12.950   \n","\n","    measurement_5  measurement_6  measurement_7  measurement_8  measurement_9  \\\n","id                                                                              \n","0          15.748         19.292         11.739         20.155         10.672   \n","1          17.717         17.893         12.748         17.889         12.448   \n","2          16.738         18.240         12.718         18.288         12.715   \n","3          18.576         18.339         12.583         19.060         12.471   \n","4          16.990         15.746         11.306         18.093         10.337   \n","\n","    measurement_10  measurement_11  measurement_12  measurement_13  \\\n","id                                                                   \n","0           15.859          17.594          15.193          15.029   \n","1           17.947          17.915          11.755          14.732   \n","2           15.607             NaN          13.798          16.711   \n","3           16.346          18.377          10.020          15.250   \n","4           17.082          19.932          12.428          16.182   \n","\n","    measurement_14  measurement_15  measurement_16  measurement_17  failure  \n","id                                                                           \n","0              NaN          13.034          14.684         764.100        0  \n","1           15.425          14.395          15.631         682.057        0  \n","2           18.631          14.094          17.946         663.376        0  \n","3           15.562          16.154          17.172         826.282        0  \n","4           12.760          13.153          16.412         579.885        0  "]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["train.head()"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>product_code</th>\n","      <th>loading</th>\n","      <th>attribute_0</th>\n","      <th>attribute_1</th>\n","      <th>attribute_2</th>\n","      <th>attribute_3</th>\n","      <th>measurement_0</th>\n","      <th>measurement_1</th>\n","      <th>measurement_2</th>\n","      <th>measurement_3</th>\n","      <th>measurement_4</th>\n","      <th>measurement_5</th>\n","      <th>measurement_6</th>\n","      <th>measurement_7</th>\n","      <th>measurement_8</th>\n","      <th>measurement_9</th>\n","      <th>measurement_10</th>\n","      <th>measurement_11</th>\n","      <th>measurement_12</th>\n","      <th>measurement_13</th>\n","      <th>measurement_14</th>\n","      <th>measurement_15</th>\n","      <th>measurement_16</th>\n","      <th>measurement_17</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>26570</th>\n","      <td>F</td>\n","      <td>119.57</td>\n","      <td>material_5</td>\n","      <td>material_6</td>\n","      <td>6</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>9</td>\n","      <td>6</td>\n","      <td>19.305</td>\n","      <td>10.178</td>\n","      <td>17.534</td>\n","      <td>18.168</td>\n","      <td>11.598</td>\n","      <td>18.654</td>\n","      <td>10.802</td>\n","      <td>15.909</td>\n","      <td>18.070</td>\n","      <td>13.772</td>\n","      <td>13.659</td>\n","      <td>16.825</td>\n","      <td>13.742</td>\n","      <td>17.710</td>\n","      <td>634.612</td>\n","    </tr>\n","    <tr>\n","      <th>26571</th>\n","      <td>F</td>\n","      <td>113.51</td>\n","      <td>material_5</td>\n","      <td>material_6</td>\n","      <td>6</td>\n","      <td>4</td>\n","      <td>11</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>17.883</td>\n","      <td>11.927</td>\n","      <td>17.228</td>\n","      <td>16.033</td>\n","      <td>11.179</td>\n","      <td>19.368</td>\n","      <td>12.032</td>\n","      <td>13.998</td>\n","      <td>NaN</td>\n","      <td>12.473</td>\n","      <td>17.468</td>\n","      <td>16.708</td>\n","      <td>14.776</td>\n","      <td>14.102</td>\n","      <td>537.037</td>\n","    </tr>\n","    <tr>\n","      <th>26572</th>\n","      <td>F</td>\n","      <td>112.16</td>\n","      <td>material_5</td>\n","      <td>material_6</td>\n","      <td>6</td>\n","      <td>4</td>\n","      <td>8</td>\n","      <td>12</td>\n","      <td>4</td>\n","      <td>18.475</td>\n","      <td>10.481</td>\n","      <td>16.619</td>\n","      <td>18.189</td>\n","      <td>12.126</td>\n","      <td>17.774</td>\n","      <td>11.743</td>\n","      <td>17.046</td>\n","      <td>18.086</td>\n","      <td>10.907</td>\n","      <td>13.363</td>\n","      <td>15.737</td>\n","      <td>17.065</td>\n","      <td>16.021</td>\n","      <td>658.995</td>\n","    </tr>\n","    <tr>\n","      <th>26573</th>\n","      <td>F</td>\n","      <td>112.72</td>\n","      <td>material_5</td>\n","      <td>material_6</td>\n","      <td>6</td>\n","      <td>4</td>\n","      <td>8</td>\n","      <td>11</td>\n","      <td>10</td>\n","      <td>16.518</td>\n","      <td>10.888</td>\n","      <td>15.293</td>\n","      <td>18.592</td>\n","      <td>11.304</td>\n","      <td>18.948</td>\n","      <td>11.790</td>\n","      <td>18.165</td>\n","      <td>16.163</td>\n","      <td>10.933</td>\n","      <td>15.501</td>\n","      <td>15.667</td>\n","      <td>12.620</td>\n","      <td>16.111</td>\n","      <td>594.301</td>\n","    </tr>\n","    <tr>\n","      <th>26574</th>\n","      <td>F</td>\n","      <td>208.00</td>\n","      <td>material_5</td>\n","      <td>material_6</td>\n","      <td>6</td>\n","      <td>4</td>\n","      <td>14</td>\n","      <td>16</td>\n","      <td>8</td>\n","      <td>17.808</td>\n","      <td>12.693</td>\n","      <td>17.678</td>\n","      <td>15.814</td>\n","      <td>13.431</td>\n","      <td>19.141</td>\n","      <td>12.370</td>\n","      <td>14.578</td>\n","      <td>17.849</td>\n","      <td>11.941</td>\n","      <td>16.070</td>\n","      <td>16.183</td>\n","      <td>13.324</td>\n","      <td>17.150</td>\n","      <td>801.044</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      product_code  loading attribute_0 attribute_1  attribute_2  attribute_3  \\\n","id                                                                              \n","26570            F   119.57  material_5  material_6            6            4   \n","26571            F   113.51  material_5  material_6            6            4   \n","26572            F   112.16  material_5  material_6            6            4   \n","26573            F   112.72  material_5  material_6            6            4   \n","26574            F   208.00  material_5  material_6            6            4   \n","\n","       measurement_0  measurement_1  measurement_2  measurement_3  \\\n","id                                                                  \n","26570              6              9              6         19.305   \n","26571             11              8              0         17.883   \n","26572              8             12              4         18.475   \n","26573              8             11             10         16.518   \n","26574             14             16              8         17.808   \n","\n","       measurement_4  measurement_5  measurement_6  measurement_7  \\\n","id                                                                  \n","26570         10.178         17.534         18.168         11.598   \n","26571         11.927         17.228         16.033         11.179   \n","26572         10.481         16.619         18.189         12.126   \n","26573         10.888         15.293         18.592         11.304   \n","26574         12.693         17.678         15.814         13.431   \n","\n","       measurement_8  measurement_9  measurement_10  measurement_11  \\\n","id                                                                    \n","26570         18.654         10.802          15.909          18.070   \n","26571         19.368         12.032          13.998             NaN   \n","26572         17.774         11.743          17.046          18.086   \n","26573         18.948         11.790          18.165          16.163   \n","26574         19.141         12.370          14.578          17.849   \n","\n","       measurement_12  measurement_13  measurement_14  measurement_15  \\\n","id                                                                      \n","26570          13.772          13.659          16.825          13.742   \n","26571          12.473          17.468          16.708          14.776   \n","26572          10.907          13.363          15.737          17.065   \n","26573          10.933          15.501          15.667          12.620   \n","26574          11.941          16.070          16.183          13.324   \n","\n","       measurement_16  measurement_17  \n","id                                     \n","26570          17.710         634.612  \n","26571          14.102         537.037  \n","26572          16.021         658.995  \n","26573          16.111         594.301  \n","26574          17.150         801.044  "]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["test.head()"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["#訓練データとテストデータを結合させる\n","\n","#訓練データとテストデータを識別するためのtrainラベルを訓練データとテストデータに付与する\n","train[\"train\"] = True\n","test[\"train\"] = False\n","\n","#テストデータの目的変数を適当にセットする\n","test[\"failure\"] = 99999\n","\n","#訓練データとテストデータを結合\n","train_test = train.append(test)\n"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["False\n","False\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["#欠損値がないか確認（nullが1つもなければ以下のprint文でTrueが出力される）\n","print(np.all(train.isnull().sum() == 0))\n","print(np.all(test.isnull().sum() == 0))\n","\n","train_miss = train.isnull().sum()\n","plt.bar(train_miss.index, train_miss)\n","plt.xticks(rotation=270)\n","plt.show()\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["test_miss = test.isnull().sum()\n","plt.bar(test_miss.index, test_miss)\n","plt.xticks(rotation=270)\n","plt.show()\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n"]}],"source":["#欠損値補完（特徴量ごとの平均値を採用）\n","train_test.fillna(train_test.mean(), inplace=True)\n","\n","#欠損値がないか確認（nullが1つもなければTrueが出力される）\n","print(np.all(train_test.isnull().sum() == 0))"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["#ラベルエンコーディングを実施\n","\n","from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","le.fit(train_test[\"product_code\"])\n","train_test[\"product_code\"] = le.transform(train_test[\"product_code\"])\n","le.fit(train_test[\"attribute_0\"])\n","train_test[\"attribute_0\"] = le.transform(train_test[\"attribute_0\"])\n","le.fit(train_test[\"attribute_1\"])\n","train_test[\"attribute_1\"] = le.transform(train_test[\"attribute_1\"])\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[array([0, 1]), array([0, 1, 2, 3]), array([5, 6, 7, 8, 9], dtype=int64), array([4, 5, 6, 7, 8, 9], dtype=int64)]\n","[array([0, 1]), array([0, 1, 2, 3]), array([5, 6, 7, 8, 9], dtype=int64), array([4, 5, 6, 7, 8, 9], dtype=int64)]\n"]}],"source":["#Onehotエンコーディングを実施\n","from sklearn.preprocessing import OneHotEncoder\n","ohe = OneHotEncoder(sparse=False, drop='first')\n","col = [\"attribute_0\", \"attribute_1\", \"attribute_2\", \"attribute_3\"]\n","ohe.fit(train_test[col])\n","print(ohe.categories_)\n","ohe.fit(train_test[col])\n","print(ohe.categories_)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['measurement_3', 'measurement_4', 'measurement_9', 'measurement_10', 'measurement_11', 'measurement_12', 'measurement_13', 'measurement_14', 'measurement_15', 'measurement_16']\n","['measurement_5', 'measurement_6', 'measurement_7', 'measurement_8']\n"]}],"source":["#特徴量追加\n","\n","#mesurement3とmesurement5の欠損値の数\n","train_test['measurement_3_na'] = train_test['measurement_3'].isna().astype(int)\n","train_test['measurement_5_na'] = train_test['measurement_5'].isna().astype(int)\n","\n","#attribute_2とattribute_3の積\n","train_test['attribute_2*3'] = train_test['attribute_2'] * train_test['attribute_3']\n","\n","#['measurement_3', 'measurement_4', 'measurement_9', 'measurement_10', 'measurement_11', 'measurement_12', 'measurement_13', 'measurement_14', 'measurement_15', 'measurement_16']の平均と分散\n","meas_gr1_cols = [f\"measurement_{i:d}\" for i in list(range(3, 5)) + list(range(9, 17))]\n","print(meas_gr1_cols)\n","train_test['meas_gr1_avg'] = np.mean(train_test[meas_gr1_cols], axis=1)\n","train_test['meas_gr1_std'] = np.std(train_test[meas_gr1_cols], axis=1)\n","\n","#['measurement_5', 'measurement_6', 'measurement_7', 'measurement_8']の平均と分散\n","meas_gr2_cols = [f\"measurement_{i:d}\" for i in list(range(5, 9))]\n","print(meas_gr2_cols)\n","train_test['meas_gr2_avg'] = np.mean(train_test[meas_gr2_cols], axis=1)\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["#訓練データとテストデータを再分割（数値データの標準化前にやっておかないといけない）\n","\n","train = train_test[train_test[\"train\"] == True]\n","train= train.drop([\"train\"], axis = 1)\n","\n","test = train_test[train_test[\"train\"] == False]\n","test= test.drop([\"train\", \"failure\"], axis = 1)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["#データを標準化する列を抽出（数値変数のみ）\n","cols = [col for col in train.columns if train[col].dtypes == 'float64']\n","\n","train[cols] = (train[cols] - train[cols].mean()) / train[cols].std(ddof=0)\n","test[cols] = (test[cols] - test[cols].mean()) / test[cols].std(ddof=0)\n"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>product_code</th>\n","      <th>loading</th>\n","      <th>attribute_0</th>\n","      <th>attribute_1</th>\n","      <th>attribute_2</th>\n","      <th>attribute_3</th>\n","      <th>measurement_0</th>\n","      <th>measurement_1</th>\n","      <th>measurement_2</th>\n","      <th>measurement_3</th>\n","      <th>measurement_4</th>\n","      <th>measurement_5</th>\n","      <th>measurement_6</th>\n","      <th>measurement_7</th>\n","      <th>measurement_8</th>\n","      <th>measurement_9</th>\n","      <th>measurement_10</th>\n","      <th>measurement_11</th>\n","      <th>measurement_12</th>\n","      <th>measurement_13</th>\n","      <th>measurement_14</th>\n","      <th>measurement_15</th>\n","      <th>measurement_16</th>\n","      <th>measurement_17</th>\n","      <th>failure</th>\n","      <th>measurement_3_na</th>\n","      <th>measurement_5_na</th>\n","      <th>attribute_2*3</th>\n","      <th>meas_gr1_avg</th>\n","      <th>meas_gr1_std</th>\n","      <th>meas_gr2_avg</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>-1.228605</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>9</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>8</td>\n","      <td>4</td>\n","      <td>0.249966</td>\n","      <td>0.797271</td>\n","      <td>-1.402872</td>\n","      <td>1.815806</td>\n","      <td>0.022852</td>\n","      <td>1.143352</td>\n","      <td>-0.777294</td>\n","      <td>-0.188783</td>\n","      <td>-1.062011</td>\n","      <td>2.414349</td>\n","      <td>-0.561228</td>\n","      <td>0.021428</td>\n","      <td>-1.319569</td>\n","      <td>-1.087242</td>\n","      <td>0.532956</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>45</td>\n","      <td>-0.581121</td>\n","      <td>-1.719996</td>\n","      <td>0.788892</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>-1.105295</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>9</td>\n","      <td>5</td>\n","      <td>14</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>0.424014</td>\n","      <td>-0.194686</td>\n","      <td>0.598880</td>\n","      <td>0.389606</td>\n","      <td>1.049291</td>\n","      <td>-1.149054</td>\n","      <td>1.042797</td>\n","      <td>1.334055</td>\n","      <td>-0.844900</td>\n","      <td>0.032516</td>\n","      <td>-0.827350</td>\n","      <td>-0.435072</td>\n","      <td>-0.405864</td>\n","      <td>-0.509723</td>\n","      <td>-0.163014</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>45</td>\n","      <td>-0.268776</td>\n","      <td>-1.076143</td>\n","      <td>0.440113</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>-1.168623</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>9</td>\n","      <td>5</td>\n","      <td>12</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>0.267069</td>\n","      <td>-0.081087</td>\n","      <td>-0.396404</td>\n","      <td>0.743352</td>\n","      <td>1.018772</td>\n","      <td>-0.745405</td>\n","      <td>1.316425</td>\n","      <td>-0.372574</td>\n","      <td>-0.091301</td>\n","      <td>1.447898</td>\n","      <td>0.945900</td>\n","      <td>1.793880</td>\n","      <td>-0.607940</td>\n","      <td>0.902056</td>\n","      <td>-0.321485</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>45</td>\n","      <td>1.716319</td>\n","      <td>-0.796378</td>\n","      <td>0.306592</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>-0.688770</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>9</td>\n","      <td>5</td>\n","      <td>13</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>-0.499549</td>\n","      <td>-0.551709</td>\n","      <td>1.472169</td>\n","      <td>0.844277</td>\n","      <td>0.881439</td>\n","      <td>0.035591</td>\n","      <td>1.066368</td>\n","      <td>0.166400</td>\n","      <td>-0.532423</td>\n","      <td>-1.169485</td>\n","      <td>-0.363204</td>\n","      <td>-0.339823</td>\n","      <td>0.775037</td>\n","      <td>0.430040</td>\n","      <td>1.060445</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>45</td>\n","      <td>-0.308771</td>\n","      <td>-0.380717</td>\n","      <td>1.613371</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>1.550629</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>9</td>\n","      <td>5</td>\n","      <td>9</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>1.563880</td>\n","      <td>1.235436</td>\n","      <td>-0.140213</td>\n","      <td>-1.799138</td>\n","      <td>-0.417631</td>\n","      <td>-0.942677</td>\n","      <td>-1.120611</td>\n","      <td>0.703186</td>\n","      <td>0.519313</td>\n","      <td>0.498768</td>\n","      <td>0.471899</td>\n","      <td>-2.287897</td>\n","      <td>-1.239678</td>\n","      <td>-0.033438</td>\n","      <td>-1.029739</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>45</td>\n","      <td>-0.128794</td>\n","      <td>0.622395</td>\n","      <td>-1.647485</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>26565</th>\n","      <td>4</td>\n","      <td>0.801245</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>9</td>\n","      <td>6</td>\n","      <td>16</td>\n","      <td>4</td>\n","      <td>-1.499572</td>\n","      <td>1.548846</td>\n","      <td>0.955719</td>\n","      <td>-2.044824</td>\n","      <td>-0.866253</td>\n","      <td>0.333017</td>\n","      <td>-0.005500</td>\n","      <td>-2.874170</td>\n","      <td>-0.826638</td>\n","      <td>-1.105748</td>\n","      <td>0.125134</td>\n","      <td>1.752861</td>\n","      <td>0.768324</td>\n","      <td>0.043155</td>\n","      <td>0.236314</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>54</td>\n","      <td>-0.687110</td>\n","      <td>-0.174197</td>\n","      <td>-0.806252</td>\n","    </tr>\n","    <tr>\n","      <th>26566</th>\n","      <td>4</td>\n","      <td>0.468386</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>9</td>\n","      <td>10</td>\n","      <td>12</td>\n","      <td>8</td>\n","      <td>-0.250046</td>\n","      <td>-0.002031</td>\n","      <td>0.870322</td>\n","      <td>1.597645</td>\n","      <td>-0.587517</td>\n","      <td>0.544453</td>\n","      <td>-0.193144</td>\n","      <td>-1.414055</td>\n","      <td>0.946772</td>\n","      <td>-1.021227</td>\n","      <td>-1.080031</td>\n","      <td>-1.201926</td>\n","      <td>-1.244378</td>\n","      <td>0.002542</td>\n","      <td>1.294932</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>54</td>\n","      <td>-1.743300</td>\n","      <td>0.459340</td>\n","      <td>1.210270</td>\n","    </tr>\n","    <tr>\n","      <th>26567</th>\n","      <td>4</td>\n","      <td>-0.314207</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>9</td>\n","      <td>1</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>-2.134396</td>\n","      <td>-0.199757</td>\n","      <td>-0.355739</td>\n","      <td>0.891171</td>\n","      <td>-0.088031</td>\n","      <td>0.257143</td>\n","      <td>-0.024047</td>\n","      <td>0.232769</td>\n","      <td>-1.141821</td>\n","      <td>-2.106145</td>\n","      <td>-0.525387</td>\n","      <td>0.381145</td>\n","      <td>0.396397</td>\n","      <td>-1.464733</td>\n","      <td>0.416433</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>54</td>\n","      <td>-2.072160</td>\n","      <td>-0.320268</td>\n","      <td>0.351268</td>\n","    </tr>\n","    <tr>\n","      <th>26568</th>\n","      <td>4</td>\n","      <td>-0.552074</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>9</td>\n","      <td>2</td>\n","      <td>9</td>\n","      <td>4</td>\n","      <td>0.269081</td>\n","      <td>-0.002031</td>\n","      <td>-0.213410</td>\n","      <td>0.601650</td>\n","      <td>-0.003597</td>\n","      <td>0.337064</td>\n","      <td>-0.039420</td>\n","      <td>0.690058</td>\n","      <td>-0.913212</td>\n","      <td>2.230758</td>\n","      <td>0.553438</td>\n","      <td>-0.393357</td>\n","      <td>-0.923474</td>\n","      <td>-2.196541</td>\n","      <td>0.245009</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>54</td>\n","      <td>-0.453586</td>\n","      <td>-1.306056</td>\n","      <td>0.360406</td>\n","    </tr>\n","    <tr>\n","      <th>26569</th>\n","      <td>4</td>\n","      <td>0.086872</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>9</td>\n","      <td>6</td>\n","      <td>19</td>\n","      <td>1</td>\n","      <td>0.243930</td>\n","      <td>-0.305241</td>\n","      <td>-0.213410</td>\n","      <td>-0.389249</td>\n","      <td>1.013686</td>\n","      <td>-0.297242</td>\n","      <td>-0.839809</td>\n","      <td>-0.375491</td>\n","      <td>0.364428</td>\n","      <td>-0.486388</td>\n","      <td>0.196816</td>\n","      <td>-1.866579</td>\n","      <td>1.025450</td>\n","      <td>0.867905</td>\n","      <td>-0.839134</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>54</td>\n","      <td>-0.257934</td>\n","      <td>0.704555</td>\n","      <td>0.056303</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>26570 rows × 31 columns</p>\n","</div>"],"text/plain":["       product_code   loading  attribute_0  attribute_1  attribute_2  \\\n","id                                                                     \n","0                 0 -1.228605            1            3            9   \n","1                 0 -1.105295            1            3            9   \n","2                 0 -1.168623            1            3            9   \n","3                 0 -0.688770            1            3            9   \n","4                 0  1.550629            1            3            9   \n","...             ...       ...          ...          ...          ...   \n","26565             4  0.801245            1            1            6   \n","26566             4  0.468386            1            1            6   \n","26567             4 -0.314207            1            1            6   \n","26568             4 -0.552074            1            1            6   \n","26569             4  0.086872            1            1            6   \n","\n","       attribute_3  measurement_0  measurement_1  measurement_2  \\\n","id                                                                \n","0                5              7              8              4   \n","1                5             14              3              3   \n","2                5             12              1              5   \n","3                5             13              2              6   \n","4                5              9              2              8   \n","...            ...            ...            ...            ...   \n","26565            9              6             16              4   \n","26566            9             10             12              8   \n","26567            9              1             10              1   \n","26568            9              2              9              4   \n","26569            9              6             19              1   \n","\n","       measurement_3  measurement_4  measurement_5  measurement_6  \\\n","id                                                                  \n","0           0.249966       0.797271      -1.402872       1.815806   \n","1           0.424014      -0.194686       0.598880       0.389606   \n","2           0.267069      -0.081087      -0.396404       0.743352   \n","3          -0.499549      -0.551709       1.472169       0.844277   \n","4           1.563880       1.235436      -0.140213      -1.799138   \n","...              ...            ...            ...            ...   \n","26565      -1.499572       1.548846       0.955719      -2.044824   \n","26566      -0.250046      -0.002031       0.870322       1.597645   \n","26567      -2.134396      -0.199757      -0.355739       0.891171   \n","26568       0.269081      -0.002031      -0.213410       0.601650   \n","26569       0.243930      -0.305241      -0.213410      -0.389249   \n","\n","       measurement_7  measurement_8  measurement_9  measurement_10  \\\n","id                                                                   \n","0           0.022852       1.143352      -0.777294       -0.188783   \n","1           1.049291      -1.149054       1.042797        1.334055   \n","2           1.018772      -0.745405       1.316425       -0.372574   \n","3           0.881439       0.035591       1.066368        0.166400   \n","4          -0.417631      -0.942677      -1.120611        0.703186   \n","...              ...            ...            ...             ...   \n","26565      -0.866253       0.333017      -0.005500       -2.874170   \n","26566      -0.587517       0.544453      -0.193144       -1.414055   \n","26567      -0.088031       0.257143      -0.024047        0.232769   \n","26568      -0.003597       0.337064      -0.039420        0.690058   \n","26569       1.013686      -0.297242      -0.839809       -0.375491   \n","\n","       measurement_11  measurement_12  measurement_13  measurement_14  \\\n","id                                                                      \n","0           -1.062011        2.414349       -0.561228        0.021428   \n","1           -0.844900        0.032516       -0.827350       -0.435072   \n","2           -0.091301        1.447898        0.945900        1.793880   \n","3           -0.532423       -1.169485       -0.363204       -0.339823   \n","4            0.519313        0.498768        0.471899       -2.287897   \n","...               ...             ...             ...             ...   \n","26565       -0.826638       -1.105748        0.125134        1.752861   \n","26566        0.946772       -1.021227       -1.080031       -1.201926   \n","26567       -1.141821       -2.106145       -0.525387        0.381145   \n","26568       -0.913212        2.230758        0.553438       -0.393357   \n","26569        0.364428       -0.486388        0.196816       -1.866579   \n","\n","       measurement_15  measurement_16  measurement_17  failure  \\\n","id                                                               \n","0           -1.319569       -1.087242        0.532956        0   \n","1           -0.405864       -0.509723       -0.163014        0   \n","2           -0.607940        0.902056       -0.321485        0   \n","3            0.775037        0.430040        1.060445        0   \n","4           -1.239678       -0.033438       -1.029739        0   \n","...               ...             ...             ...      ...   \n","26565        0.768324        0.043155        0.236314        0   \n","26566       -1.244378        0.002542        1.294932        0   \n","26567        0.396397       -1.464733        0.416433        0   \n","26568       -0.923474       -2.196541        0.245009        0   \n","26569        1.025450        0.867905       -0.839134        0   \n","\n","       measurement_3_na  measurement_5_na  attribute_2*3  meas_gr1_avg  \\\n","id                                                                       \n","0                     0                 0             45     -0.581121   \n","1                     0                 0             45     -0.268776   \n","2                     0                 0             45      1.716319   \n","3                     0                 0             45     -0.308771   \n","4                     0                 0             45     -0.128794   \n","...                 ...               ...            ...           ...   \n","26565                 0                 0             54     -0.687110   \n","26566                 0                 0             54     -1.743300   \n","26567                 0                 0             54     -2.072160   \n","26568                 0                 0             54     -0.453586   \n","26569                 0                 0             54     -0.257934   \n","\n","       meas_gr1_std  meas_gr2_avg  \n","id                                 \n","0         -1.719996      0.788892  \n","1         -1.076143      0.440113  \n","2         -0.796378      0.306592  \n","3         -0.380717      1.613371  \n","4          0.622395     -1.647485  \n","...             ...           ...  \n","26565     -0.174197     -0.806252  \n","26566      0.459340      1.210270  \n","26567     -0.320268      0.351268  \n","26568     -1.306056      0.360406  \n","26569      0.704555      0.056303  \n","\n","[26570 rows x 31 columns]"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["train"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>product_code</th>\n","      <th>loading</th>\n","      <th>attribute_0</th>\n","      <th>attribute_1</th>\n","      <th>attribute_2</th>\n","      <th>attribute_3</th>\n","      <th>measurement_0</th>\n","      <th>measurement_1</th>\n","      <th>measurement_2</th>\n","      <th>measurement_3</th>\n","      <th>measurement_4</th>\n","      <th>measurement_5</th>\n","      <th>measurement_6</th>\n","      <th>measurement_7</th>\n","      <th>measurement_8</th>\n","      <th>measurement_9</th>\n","      <th>measurement_10</th>\n","      <th>measurement_11</th>\n","      <th>measurement_12</th>\n","      <th>measurement_13</th>\n","      <th>measurement_14</th>\n","      <th>measurement_15</th>\n","      <th>measurement_16</th>\n","      <th>measurement_17</th>\n","      <th>measurement_3_na</th>\n","      <th>measurement_5_na</th>\n","      <th>attribute_2*3</th>\n","      <th>meas_gr1_avg</th>\n","      <th>meas_gr1_std</th>\n","      <th>meas_gr2_avg</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>26570</th>\n","      <td>5</td>\n","      <td>-0.207125</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>9</td>\n","      <td>6</td>\n","      <td>1.520347</td>\n","      <td>-1.554278</td>\n","      <td>0.397540</td>\n","      <td>0.662282</td>\n","      <td>-0.114918</td>\n","      <td>-0.382481</td>\n","      <td>-0.630140</td>\n","      <td>-0.140888</td>\n","      <td>-0.509061</td>\n","      <td>1.410415</td>\n","      <td>-1.578778</td>\n","      <td>0.517322</td>\n","      <td>-0.920774</td>\n","      <td>0.686537</td>\n","      <td>-0.535760</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>24</td>\n","      <td>-0.348016</td>\n","      <td>0.433461</td>\n","      <td>0.283021</td>\n","    </tr>\n","    <tr>\n","      <th>26571</th>\n","      <td>5</td>\n","      <td>-0.362737</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>4</td>\n","      <td>11</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>0.090072</td>\n","      <td>0.200249</td>\n","      <td>0.090397</td>\n","      <td>-1.505431</td>\n","      <td>-0.540917</td>\n","      <td>0.342618</td>\n","      <td>0.627617</td>\n","      <td>-1.394294</td>\n","      <td>0.111735</td>\n","      <td>0.427967</td>\n","      <td>1.320470</td>\n","      <td>0.431343</td>\n","      <td>-0.225269</td>\n","      <td>-1.602976</td>\n","      <td>-1.318673</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>24</td>\n","      <td>-0.246046</td>\n","      <td>-0.784759</td>\n","      <td>-0.804096</td>\n","    </tr>\n","    <tr>\n","      <th>26572</th>\n","      <td>5</td>\n","      <td>-0.397403</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>4</td>\n","      <td>8</td>\n","      <td>12</td>\n","      <td>4</td>\n","      <td>0.685517</td>\n","      <td>-1.250321</td>\n","      <td>-0.520876</td>\n","      <td>0.683604</td>\n","      <td>0.421901</td>\n","      <td>-1.276161</td>\n","      <td>0.332096</td>\n","      <td>0.604859</td>\n","      <td>-0.498705</td>\n","      <td>-0.756415</td>\n","      <td>-1.804081</td>\n","      <td>-0.282201</td>\n","      <td>1.314393</td>\n","      <td>-0.385245</td>\n","      <td>-0.340118</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>24</td>\n","      <td>-0.599934</td>\n","      <td>0.287864</td>\n","      <td>-0.348176</td>\n","    </tr>\n","    <tr>\n","      <th>26573</th>\n","      <td>5</td>\n","      <td>-0.383023</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>4</td>\n","      <td>8</td>\n","      <td>11</td>\n","      <td>10</td>\n","      <td>-1.282872</td>\n","      <td>-0.842035</td>\n","      <td>-1.851826</td>\n","      <td>1.092779</td>\n","      <td>-0.413829</td>\n","      <td>-0.083911</td>\n","      <td>0.380156</td>\n","      <td>1.338800</td>\n","      <td>-1.743272</td>\n","      <td>-0.736751</td>\n","      <td>-0.176727</td>\n","      <td>-0.333641</td>\n","      <td>-1.675471</td>\n","      <td>-0.328134</td>\n","      <td>-0.859204</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>24</td>\n","      <td>-1.697990</td>\n","      <td>-0.690117</td>\n","      <td>-0.637432</td>\n","    </tr>\n","    <tr>\n","      <th>26574</th>\n","      <td>5</td>\n","      <td>2.063631</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>4</td>\n","      <td>14</td>\n","      <td>16</td>\n","      <td>8</td>\n","      <td>0.014635</td>\n","      <td>0.968670</td>\n","      <td>0.542077</td>\n","      <td>-1.727787</td>\n","      <td>1.748699</td>\n","      <td>0.112089</td>\n","      <td>0.973245</td>\n","      <td>-1.013878</td>\n","      <td>-0.652092</td>\n","      <td>0.025610</td>\n","      <td>0.256372</td>\n","      <td>0.045545</td>\n","      <td>-1.201935</td>\n","      <td>0.331180</td>\n","      <td>0.799641</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>24</td>\n","      <td>-0.349458</td>\n","      <td>-1.473109</td>\n","      <td>0.338745</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>47340</th>\n","      <td>8</td>\n","      <td>0.439205</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>9</td>\n","      <td>0.675459</td>\n","      <td>0.845281</td>\n","      <td>1.011824</td>\n","      <td>-0.433251</td>\n","      <td>-0.515499</td>\n","      <td>-0.464741</td>\n","      <td>0.278922</td>\n","      <td>-0.002194</td>\n","      <td>0.591827</td>\n","      <td>-0.261032</td>\n","      <td>1.151493</td>\n","      <td>-0.727523</td>\n","      <td>0.065982</td>\n","      <td>-0.057431</td>\n","      <td>-0.039462</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>45</td>\n","      <td>0.693083</td>\n","      <td>-0.261491</td>\n","      <td>-0.194176</td>\n","    </tr>\n","    <tr>\n","      <th>47341</th>\n","      <td>8</td>\n","      <td>-1.363687</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>8</td>\n","      <td>7</td>\n","      <td>1.112990</td>\n","      <td>-1.837170</td>\n","      <td>1.154354</td>\n","      <td>-0.002785</td>\n","      <td>0.036571</td>\n","      <td>-1.130938</td>\n","      <td>-0.448123</td>\n","      <td>-0.063493</td>\n","      <td>-2.046809</td>\n","      <td>1.253102</td>\n","      <td>-0.182055</td>\n","      <td>-0.606272</td>\n","      <td>-0.873689</td>\n","      <td>-0.080653</td>\n","      <td>-0.707171</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>45</td>\n","      <td>-1.263383</td>\n","      <td>-0.601537</td>\n","      <td>0.035289</td>\n","    </tr>\n","    <tr>\n","      <th>47342</th>\n","      <td>8</td>\n","      <td>-1.538301</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>5</td>\n","      <td>10</td>\n","      <td>11</td>\n","      <td>2</td>\n","      <td>0.867570</td>\n","      <td>0.002570</td>\n","      <td>1.108182</td>\n","      <td>0.400329</td>\n","      <td>0.376150</td>\n","      <td>0.608691</td>\n","      <td>-1.004399</td>\n","      <td>0.008655</td>\n","      <td>1.136123</td>\n","      <td>-0.584733</td>\n","      <td>0.598131</td>\n","      <td>-0.437990</td>\n","      <td>-0.629523</td>\n","      <td>0.697959</td>\n","      <td>0.657662</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>45</td>\n","      <td>0.296189</td>\n","      <td>1.171680</td>\n","      <td>1.250080</td>\n","    </tr>\n","    <tr>\n","      <th>47343</th>\n","      <td>8</td>\n","      <td>-0.038160</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>5</td>\n","      <td>8</td>\n","      <td>16</td>\n","      <td>11</td>\n","      <td>-1.264767</td>\n","      <td>-0.502967</td>\n","      <td>1.009817</td>\n","      <td>-0.269784</td>\n","      <td>0.997355</td>\n","      <td>0.552836</td>\n","      <td>1.282060</td>\n","      <td>-0.460307</td>\n","      <td>0.413847</td>\n","      <td>-1.957436</td>\n","      <td>0.063799</td>\n","      <td>0.942068</td>\n","      <td>0.891978</td>\n","      <td>-0.919549</td>\n","      <td>0.351646</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>45</td>\n","      <td>-0.459793</td>\n","      <td>0.343057</td>\n","      <td>1.147751</td>\n","    </tr>\n","    <tr>\n","      <th>47344</th>\n","      <td>8</td>\n","      <td>-1.069924</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>0.582923</td>\n","      <td>-0.519017</td>\n","      <td>-0.108342</td>\n","      <td>0.903929</td>\n","      <td>-1.555586</td>\n","      <td>0.225830</td>\n","      <td>-1.473757</td>\n","      <td>0.751778</td>\n","      <td>-0.603552</td>\n","      <td>1.375624</td>\n","      <td>-0.858723</td>\n","      <td>0.983955</td>\n","      <td>0.773594</td>\n","      <td>-0.092710</td>\n","      <td>-0.428499</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>45</td>\n","      <td>0.449084</td>\n","      <td>-0.025835</td>\n","      <td>-0.266110</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>20775 rows × 30 columns</p>\n","</div>"],"text/plain":["       product_code   loading  attribute_0  attribute_1  attribute_2  \\\n","id                                                                     \n","26570             5 -0.207125            0            1            6   \n","26571             5 -0.362737            0            1            6   \n","26572             5 -0.397403            0            1            6   \n","26573             5 -0.383023            0            1            6   \n","26574             5  2.063631            0            1            6   \n","...             ...       ...          ...          ...          ...   \n","47340             8  0.439205            1            0            9   \n","47341             8 -1.363687            1            0            9   \n","47342             8 -1.538301            1            0            9   \n","47343             8 -0.038160            1            0            9   \n","47344             8 -1.069924            1            0            9   \n","\n","       attribute_3  measurement_0  measurement_1  measurement_2  \\\n","id                                                                \n","26570            4              6              9              6   \n","26571            4             11              8              0   \n","26572            4              8             12              4   \n","26573            4              8             11             10   \n","26574            4             14             16              8   \n","...            ...            ...            ...            ...   \n","47340            5              0              4              9   \n","47341            5              4              8              7   \n","47342            5             10             11              2   \n","47343            5              8             16             11   \n","47344            5              0             11             11   \n","\n","       measurement_3  measurement_4  measurement_5  measurement_6  \\\n","id                                                                  \n","26570       1.520347      -1.554278       0.397540       0.662282   \n","26571       0.090072       0.200249       0.090397      -1.505431   \n","26572       0.685517      -1.250321      -0.520876       0.683604   \n","26573      -1.282872      -0.842035      -1.851826       1.092779   \n","26574       0.014635       0.968670       0.542077      -1.727787   \n","...              ...            ...            ...            ...   \n","47340       0.675459       0.845281       1.011824      -0.433251   \n","47341       1.112990      -1.837170       1.154354      -0.002785   \n","47342       0.867570       0.002570       1.108182       0.400329   \n","47343      -1.264767      -0.502967       1.009817      -0.269784   \n","47344       0.582923      -0.519017      -0.108342       0.903929   \n","\n","       measurement_7  measurement_8  measurement_9  measurement_10  \\\n","id                                                                   \n","26570      -0.114918      -0.382481      -0.630140       -0.140888   \n","26571      -0.540917       0.342618       0.627617       -1.394294   \n","26572       0.421901      -1.276161       0.332096        0.604859   \n","26573      -0.413829      -0.083911       0.380156        1.338800   \n","26574       1.748699       0.112089       0.973245       -1.013878   \n","...              ...            ...            ...             ...   \n","47340      -0.515499      -0.464741       0.278922       -0.002194   \n","47341       0.036571      -1.130938      -0.448123       -0.063493   \n","47342       0.376150       0.608691      -1.004399        0.008655   \n","47343       0.997355       0.552836       1.282060       -0.460307   \n","47344      -1.555586       0.225830      -1.473757        0.751778   \n","\n","       measurement_11  measurement_12  measurement_13  measurement_14  \\\n","id                                                                      \n","26570       -0.509061        1.410415       -1.578778        0.517322   \n","26571        0.111735        0.427967        1.320470        0.431343   \n","26572       -0.498705       -0.756415       -1.804081       -0.282201   \n","26573       -1.743272       -0.736751       -0.176727       -0.333641   \n","26574       -0.652092        0.025610        0.256372        0.045545   \n","...               ...             ...             ...             ...   \n","47340        0.591827       -0.261032        1.151493       -0.727523   \n","47341       -2.046809        1.253102       -0.182055       -0.606272   \n","47342        1.136123       -0.584733        0.598131       -0.437990   \n","47343        0.413847       -1.957436        0.063799        0.942068   \n","47344       -0.603552        1.375624       -0.858723        0.983955   \n","\n","       measurement_15  measurement_16  measurement_17  measurement_3_na  \\\n","id                                                                        \n","26570       -0.920774        0.686537       -0.535760                 0   \n","26571       -0.225269       -1.602976       -1.318673                 0   \n","26572        1.314393       -0.385245       -0.340118                 0   \n","26573       -1.675471       -0.328134       -0.859204                 0   \n","26574       -1.201935        0.331180        0.799641                 0   \n","...               ...             ...             ...               ...   \n","47340        0.065982       -0.057431       -0.039462                 0   \n","47341       -0.873689       -0.080653       -0.707171                 0   \n","47342       -0.629523        0.697959        0.657662                 0   \n","47343        0.891978       -0.919549        0.351646                 0   \n","47344        0.773594       -0.092710       -0.428499                 0   \n","\n","       measurement_5_na  attribute_2*3  meas_gr1_avg  meas_gr1_std  \\\n","id                                                                   \n","26570                 0             24     -0.348016      0.433461   \n","26571                 0             24     -0.246046     -0.784759   \n","26572                 0             24     -0.599934      0.287864   \n","26573                 0             24     -1.697990     -0.690117   \n","26574                 0             24     -0.349458     -1.473109   \n","...                 ...            ...           ...           ...   \n","47340                 0             45      0.693083     -0.261491   \n","47341                 0             45     -1.263383     -0.601537   \n","47342                 0             45      0.296189      1.171680   \n","47343                 0             45     -0.459793      0.343057   \n","47344                 0             45      0.449084     -0.025835   \n","\n","       meas_gr2_avg  \n","id                   \n","26570      0.283021  \n","26571     -0.804096  \n","26572     -0.348176  \n","26573     -0.637432  \n","26574      0.338745  \n","...             ...  \n","47340     -0.194176  \n","47341      0.035289  \n","47342      1.250080  \n","47343      1.147751  \n","47344     -0.266110  \n","\n","[20775 rows x 30 columns]"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["test"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["auc_list = []\n","test_pred_list = []\n","importance_list = []\n","kf = GroupKFold(n_splits=5)   # grouped by product_code\n"]},{"cell_type":"code","execution_count":142,"metadata":{},"outputs":[],"source":["#訓練データの分割\n","train_label = train[\"failure\"]\n","train = train.drop([\"failure\"], axis=1)\n","train_X, val_X, train_Y, val_Y = train_test_split(\n","    train, train_label, test_size=0.1, random_state=42, stratify=train_label)"]},{"cell_type":"code","execution_count":143,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n","    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n","    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n","ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n","    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n","ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n","    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n","    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n","ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n","    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n","ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n","    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n","    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n","ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n","    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n","ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n","    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n","    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n","ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n","    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n","ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n","    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n","    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n","ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n","    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n","ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\"The max_iter was reached which means \"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\"The max_iter was reached which means \"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\"The max_iter was reached which means \"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\"The max_iter was reached which means \"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\"The max_iter was reached which means \"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n","    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n","    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n","ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n","    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n","ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\"The max_iter was reached which means \"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\"The max_iter was reached which means \"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\"The max_iter was reached which means \"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\"The max_iter was reached which means \"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\"The max_iter was reached which means \"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n","    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n","    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n","ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","Traceback (most recent call last):\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n","    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n","ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n","c:\\Users\\keisu\\anaconda3\\envs\\myenv2\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\"Liblinear failed to converge, increase \"\n"]},{"name":"stdout","output_type":"stream","text":["{'C': 0.001, 'penalty': 'l2', 'solver': 'lbfgs'}\n"]}],"source":["\n","# チューニングモデル\n","logreg = LogisticRegression()\n","\n","param_grid = {'C': [0.0001, 0.001, 0.01, 0.05 ,0.1, 0.5, 1],\n","                 'penalty':['l2','l1'],\n","                 'solver': ['lbfgs', 'sag', 'newton-cg', 'liblinear']}  \n","logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n","logreg_cv.fit(train_X, train_Y)\n","print(logreg_cv.best_params_)\n"]},{"cell_type":"code","execution_count":144,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.79      1.00      0.88      2092\n","           1       0.00      0.00      0.00       565\n","\n","    accuracy                           0.79      2657\n","   macro avg       0.39      0.50      0.44      2657\n","weighted avg       0.62      0.79      0.69      2657\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","predicted = logreg_cv.predict(val_X)\n","print(classification_report(val_Y, predicted))\n"]},{"cell_type":"code","execution_count":145,"metadata":{},"outputs":[{"data":{"text/plain":["array([0.22626233, 0.18558803, 0.20924028, ..., 0.17644424, 0.25152409,\n","       0.19644711])"]},"execution_count":145,"metadata":{},"output_type":"execute_result"}],"source":["pred_y = logreg_cv.predict_proba(test)[:, 1]\n","pred_y\n"]},{"cell_type":"code","execution_count":146,"metadata":{},"outputs":[],"source":["submission = pd.read_csv(\"../submission/sample_submission.csv\")\n","submission[\"failure\"] = pred_y\n","submission.to_csv(\"../submission/submission.csv\", index=False)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.5 ('myenv2')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"vscode":{"interpreter":{"hash":"a592d3333deeac49ae2bcb48983e127e6be61f9a7c8f4f8723a33a6748dd23db"}}},"nbformat":4,"nbformat_minor":4}
